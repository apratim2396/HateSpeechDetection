# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RXGBtLC91QflZC3BzTzmYyGzU5bFchXh

Importing Basic Libraries
"""

import sklearn
import numpy as np
import pandas as pd

"""Importing Data"""

train=pd.read_csv("/content/train.csv")

test=pd.read_csv("/content/test.csv")

"""Data Exploration"""

train.head()

test.tail()

#no of non racist/sexist tweets
sum(train["label"]==0)

#no of racist/sexist tweets
sum(train["label"]==1)

#checking if there are any entries left as empty
train.isnull().sum()

"""Cleaning The Data"""

#installing tweet preprocessor to clean tweets
!pip install tweet-preprocessor

#removing special characters from the tweets using regular expression library
import re

#setting up the regex(punctuation) we want to be removed
REPLACE_NO_SPACE=re.compile("(\.)|(\;)|(\:)|(\!)|(\')|(\?)|(\,)|(\")|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\$)|(\>)|(\<)|(\{)|(\})")
REPLACE_WITH_SPACE=re.compile("(<br\s/><br\s/?)|(-)|(/)|(:).")

import preprocessor as p

#creating a custom function to clean the dataset(combining pre processor and regular expression library)
def clean_tweets(df):
  tempArr = []
  for line in df:
    # send to tweet_processor
    tmpL = p.clean(line)
    # remove puctuation
    tmpL = REPLACE_NO_SPACE.sub("", tmpL.lower()) # convert all tweets to lower cases
    tmpL = REPLACE_WITH_SPACE.sub(" ", tmpL)
    tempArr.append(tmpL)
  return tempArr

# cleaning the training data
train_tweet = clean_tweets(train["tweet"])
train_tweet = pd.DataFrame(train_tweet)

# appending cleaned tweets to the training data
train["clean_tweet"] = train_tweet

# compare the cleaned and uncleaned tweets
train.head(10)

# clean the test data and append the cleaned tweets to the test data
test_tweet = clean_tweets(test["tweet"])
test_tweet = pd.DataFrame(test_tweet)
# append cleaned tweets to the training data
test["clean_tweet"] = test_tweet

# compare the cleaned and uncleaned tweets
test.tail()

"""Splitting the Data into Train and Test"""

from sklearn.model_selection import train_test_split

# extract the labels from the train data
y = train.label.values

# use 70% for the training and 30% for the test
x_train, x_test, y_train, y_test = train_test_split(train.clean_tweet.values, y, 
                                                    stratify=y, 
                                                    random_state=1, 
                                                    test_size=0.3, shuffle=True)

"""Vectorize using the CountVectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

documents = ["This is Import Data's ",
             "Computer Science and Machine Learning is fun!",
             "Have a nice day ahead"]

# initializing the countvectorizer
vectorizer = CountVectorizer()

# tokenize and make the document into a matrix
document_term_matrix = vectorizer.fit_transform(documents)

# check the result
pd.DataFrame(document_term_matrix.toarray(), columns = vectorizer.get_feature_names())

from sklearn.feature_extraction.text import CountVectorizer

# vectorize tweets for model building
vectorizer = CountVectorizer(binary=True, stop_words='english')

# learn a vocabulary dictionary of all tokens in the raw documents
vectorizer.fit(list(x_train) + list(x_test))

# transform documents to document-term matrix
x_train_vec = vectorizer.transform(x_train)
x_test_vec = vectorizer.transform(x_test)

"""Building the Model"""

from sklearn import svm
# classify using support vector classifier
svm = svm.SVC(kernel = 'linear', probability=True)

# fit the SVC model based on the given training data
prob = svm.fit(x_train_vec, y_train).predict_proba(x_test_vec)

# perform classification and prediction on samples in x_test
y_pred_svm = svm.predict(x_test_vec)

"""Accuracy Score for SVC"""

from sklearn.metrics import accuracy_score
print("Accuracy score of the model is: ", accuracy_score(y_test, y_pred_svm) * 100, '%')